{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "  - ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KaVwN269EttM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "397720.33s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "42b2ba5e-11ae-4f4d-e68e-77607bb794ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "30aa2260-50f1-4abf-b305-eed0ee1b9008"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method.\n",
        "\n",
        "A: Added DuckDuckGo and Arxiv serch tools to tool_belt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun(),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "A: The model has the ability of Contextual Understanding and Intent Recognition. Based on the query and context the LLM can recognize the context and intent of the query, and from here it can recognize which is the proper tool to use. For example if the query contains words like 'research' or scientific' it will know the intent of the user is to look up information on research papers and the right tool in Arxiv which is a database for scientific papers. Something more general will likely be routed to the DuckDuckgo agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "A: No there are no specific limits for the number of cycles an agent can go through. When limits are implemented usualy cycles of less than 10 are implemented.\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "A: You can implement a While loop and set a max_agent_cycles variable. This will create a limit of how many cycles to go through."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "a5d7ef7a-13f2-4066-df52-b0df89eae2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_dbZ3TmPopSmrT1SrM0mq3OCV', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 156, 'total_tokens': 181}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d2f45abe-6d23-464a-8368-43f82e884928-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of the Winnipeg Jets 2023'}, 'id': 'call_dbZ3TmPopSmrT1SrM0mq3OCV', 'type': 'tool_call'}], usage_metadata={'input_tokens': 156, 'output_tokens': 25, 'total_tokens': 181})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Lowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ... The Winnipeg Jets will have a captain for the 2023-24 season. After going captain-less in 2022-23, the Winnipeg Jets unveiled Adam Lowry as the club\\'s new captain on Tuesday morning. \"When I ... Adam Lowry was named captain of the Winnipeg Jets on Tuesday. ... Sep 20, 2023. Latest News. Inside look at Vegas Golden Knights Aug 30, 2024. Vegas Golden Knights fantasy projections for 2024-25 Posted September 12, 2023 9:29 am. Centre Adam Lowry was named the Winnipeg Jets new captain on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta to Winnipeg in 2011. He follows Andrew Ladd and Blake Wheeler, who served as captain for five and six years respectively. September 12, 2023. There are not many honours in team sports bigger than being named captain. That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the ...', name='duckduckgo_search', tool_call_id='call_dbZ3TmPopSmrT1SrM0mq3OCV')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry. He was named the captain on September 12, 2023.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 443, 'total_tokens': 470}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-e3e49468-48f1-490d-a9dc-56cb182b1b90-0', usage_metadata={'input_tokens': 443, 'output_tokens': 27, 'total_tokens': 470})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "026a3aa3-1c3e-4016-b0a3-fe98b1d25399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_x173PbWtS0ysdznxJtAhtfm9', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_vuGcBfJoJVRxAj2iJ1XwlkWh', 'function': {'arguments': '{\"query\": \"latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 173, 'total_tokens': 223}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a4d8432f-3567-415f-98d6-5ee45974a34e-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_x173PbWtS0ysdznxJtAhtfm9', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet'}, 'id': 'call_vuGcBfJoJVRxAj2iJ1XwlkWh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 50, 'total_tokens': 223})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', tool_call_id='call_x173PbWtS0ysdznxJtAhtfm9'), ToolMessage(content=\"Introducing a new form of Free (v2) access for write-only use cases and those testing the Twitter API with 1,500 Tweets/month at the app level, media upload endpoints, and Login with Twitter. Get ... Twitter. The latest Twitter news and updates. Twitter is a social networking service, primarily microblogging but also a picture and video sharing service, founded by Jack Dorsey, Noah Glass, Biz ... Prosecutors from the special counsel's office, for example, included tweets in which Trump pressured Vice President Mike Pence to reject Electoral College votes before Congress and tweets urging ... Twitter has added a warning to one of President Donald Trump's tweets about protests in Minneapolis. The company says the tweet violated the platform's rules about glorifying violence. U.S ... Reviews. 7NEWS brings you the latest Twitter news from Australia and around the world. Stay up to date with all of the breaking Twitter headlines. Today's Twitter news, live updates & all the latest breaking stories from 7NEWS.\", name='duckduckgo_search', tool_call_id='call_vuGcBfJoJVRxAj2iJ1XwlkWh')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UOwduWWqIXYxE1OhFDpT2UU4', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_bDnagpQps2kNvny4j3v9jDYh', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_QYfGfBv6vkcxUTqvJVcqmgmR', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_TI2pBswox4ioESB07tzSDij6', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 1402, 'total_tokens': 1510}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f4ccecb6-45b8-4047-a833-5777a67d96b2-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers latest Tweet'}, 'id': 'call_UOwduWWqIXYxE1OhFDpT2UU4', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Artidoro Pagnoni latest Tweet'}, 'id': 'call_bDnagpQps2kNvny4j3v9jDYh', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Ari Holtzman latest Tweet'}, 'id': 'call_QYfGfBv6vkcxUTqvJVcqmgmR', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Luke Zettlemoyer latest Tweet'}, 'id': 'call_TI2pBswox4ioESB07tzSDij6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1402, 'output_tokens': 108, 'total_tokens': 1510})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: duckduckgo_search\n",
            "[ToolMessage(content='\"Donald Trump is ditching workers on Labor Day because he is an anti-worker, anti-union extremist who will sell out working families for his billionaire donors if he takes power,\" Harris campaign spokesperson Joseph Costello said in a statement.. \"Vice President Harris is the only candidate for president who stands firmly on the side of labor and working Americans, and she is fighting to ... Minnesota Gov. Tim Walz was selected as Vice President Kamala Harris\\' running mate due to his use of the word \"weird\" to describe former President Trump and Sen. JD Vance, and now‚Ä¶ Minnesota Gov. Tim Walz (D) boasted on Monday that he\\'s in \"the pocket\" of unions, as he and Vice President Harris court support from the critical voting bloc in several \"blue wall\" states ... Minnesota Gov. Tim Walz\\'s trip to Wisconsin on Monday marked the first use of the Harris-Walz campaign\\'s new campaign charter plane, the campaign said. The plane bears an American flag on its ... Walz, Trump coming to Pennsylvania. Elsewhere in this battleground state, Democratic vice presidential nominee Minnesota Gov. Tim Walz will be in Pennsylvania for multiple events on Wednesday and ...', name='duckduckgo_search', tool_call_id='call_UOwduWWqIXYxE1OhFDpT2UU4'), ToolMessage(content='We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly ... efficient finetuning of quantized LLMs. AUTHORs: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer Authors Info & Claims. NIPS \\'23: Proceedings of the 37th International Conference on Neural Information Processing Systems. Article No.: 441, Pages 10088 - 10115. Published: 30 May 2024 Publication History. In this paper, we address these aforementioned challenges associated with financial data and introduce FinGPT, an end-to-end open-source framework for financial large language models (FinLLMs). Adopting a data-centric approach, FinGPT underscores the crucial role of data acquisition, cleaning, and preprocessing in developing open-source FinLLMs. The recent paper \"QLORA: Efficient Finetuning of Quantized LLMs\" by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer introduces a new method called QLORA that enables efficient ... Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models (LLMs) by enabling faster operation and compatibility with more accessible hardware through reduced memory usage, at the cost of small performance drops. We explore the role of calibration sets in PTQ, specifically their effect on hidden activations in various ...', name='duckduckgo_search', tool_call_id='call_bDnagpQps2kNvny4j3v9jDYh'), ToolMessage(content='MSNBC host Ari Melber, during an interview with Trump campaign adviser Corey Lewandowski on Wednesday, threatened him with a defamation lawsuit for quoting the anchor calling the former President ... Ari Melber warns Trump campaign advisor Corey Lewandowski he \\'will be potentially in a defamation situation\\' on Wednesday\\'s episode of The Beat (MSNBC) \"I did not say that. That is a false ... The heated on-air dispute last night between MSNBC\\'s Ari Melber and Donald Trump campaign adviser Corey Lewandowski didn\\'t end with Wednesday\\'s segment: Today, Lewandowski tweeted a video in ... The team behind QLoRA includes Allen School Ph.D. student Artidoro Pagnoni; alum Ari Holtzman (Ph.D., \\'23), incoming professor at the University of Chicago; and professor Luke Zettlemoyer, who is also a research manager at Meta. Madrona Prize First Runner Up / Punica: Multi-Tenant LoRA Fine-tuned LLM Serving Jack Hessel ‚Ä† Ari Holtzman ... When uploading an image alongside a tweet, users of Twitter have the option of providing alternative text: while few use this feature (gleason2019s find that fewer than .1% of image tweets have alt-text), its broader adoption might someday make social media more accessible for low vision and blind users. We ...', name='duckduckgo_search', tool_call_id='call_QYfGfBv6vkcxUTqvJVcqmgmR'), ToolMessage(content=\"Error: RatelimitException('https://duckduckgo.com 202 Ratelimit')\\n Please fix your mistakes.\", name='duckduckgo_search', tool_call_id='call_TI2pBswox4ioESB07tzSDij6')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"Here are the latest findings for the authors of the QLoRA paper:\\n\\n1. **Tim Dettmers**:\\n   - No specific latest tweet found. The search results included general information about QLoRA and other related topics.\\n\\n2. **Artidoro Pagnoni**:\\n   - No specific latest tweet found. The search results included general information about QLoRA and other related topics.\\n\\n3. **Ari Holtzman**:\\n   - No specific latest tweet found. The search results included general information about QLoRA and other related topics.\\n\\n4. **Luke Zettlemoyer**:\\n   - The search for Luke Zettlemoyer's latest tweet hit a rate limit error on DuckDuckGo.\\n\\nIt seems that the searches did not yield specific tweets from the authors. If you need more detailed information or specific tweets, you might want to check their individual Twitter profiles directly.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 183, 'prompt_tokens': 2433, 'total_tokens': 2616}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-028dc599-32ba-494e-92fb-19c3fb65c6e5-0', usage_metadata={'input_tokens': 2433, 'output_tokens': 183, 'total_tokens': 2616})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "\n",
        "A:\n",
        "\n",
        "Step 1 \"HumanMessage\" - The Human asked the model to use Arxiv for Qlora papers and then use DuckDuckGo to find the authors latest Tweets.\n",
        "\n",
        "Step 2 \"Receiving update from node: 'agent'\" - The agent decided to use two tools, Arxiv and DuckDuckGo search.\n",
        "\n",
        "Step 3 \"Receiving update from node: 'action'\" - Arxiv was called and used to find Qlora papers, it extracted Publishing Date, Title, Authors, and a Summary.\n",
        "\n",
        "Step 4 \"Receiving update from node: 'agent'\" - Now the AI model is now searching for the latest tweets from each author using DuckDuckGo search\n",
        "\n",
        "Step 5 \"Receiving update from node: 'action'\" - We get the Action updates from the DuckDuckGo search results; all the latest tweets from each author of the paper.\n",
        "\n",
        "Step 6 \"Receiving update from node: 'agent'\" - The AI model created a summary from all the information gathered from the tools. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "b160f3e4-89d1-4411-ed96-fe17b3cad200"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here's a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This can involve concatenating the retrieved information with the original query or using it to provide additional context.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based model like GPT-3, uses the augmented input to generate a response. The generative model can produce more accurate and contextually relevant answers because it has access to the additional information retrieved in the first step.\\n\\nRAG models are particularly useful in scenarios where the information needed to answer a query is not contained within the model's parameters but can be found in external documents or databases. This approach leverages the strengths of both retrieval and generation, leading to more accurate and informative responses.\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is a Large Language Model (LLM)?\",\n",
        "    \"What are some common use cases of LLMs?\",\n",
        "    \"Which transformer architecture is most commonly used in LLMs?\",\n",
        "    \"What is fine-tuning in the context of LLMs?\",\n",
        "    \"Who are some key contributors to the development of LLMs?\",\n",
        "    \"What is the significance of tokenization in LLMs?\",\n",
        "    \"What is the primary challenge of scaling LLMs?\",\n",
        "    \"How do LLMs handle context in conversations?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"neural\", \"natural language\"]},\n",
        "    {\"must_mention\" : [\"text generation\", \"summarization\"]},\n",
        "    {\"must_mention\" : [\"GPT\", \"BERT\"]},\n",
        "    {\"must_mention\" : [\"pre-trained\", \"specific tasks\"]},\n",
        "    {\"must_mention\" : [\"OpenAI\", \"Google\", \"Hugging Face\"]},\n",
        "    {\"must_mention\" : [\"splitting\", \"subwords\"]},\n",
        "    {\"must_mention\" : [\"computational\", \"memory\"]},\n",
        "    {\"must_mention\" : [\"context window\", \"previous inputs\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "A: \n",
        "In the set of questions and answers provided, the correct answers are associated with the questions by their position in the list. Each question in the questions list corresponds to an answer in the answers list at the same index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "A: We can add partial matching, or semantic matching. Also having a weighted scoring system could help improve the evaluation of the system. We can also add words that we do not want mentioned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "8b98fff1-bd75-4dbe-cadc-a76d8a82577c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - dfb6ce7b' at:\n",
            "https://smith.langchain.com/o/9e230557-3355-5562-8dbc-5c44b3821cac/datasets/ddde42ac-bb21-4605-8c21-d8758fecbf2d/compare?selectedSessions=8df26621-b572-4aab-9737-5d84473f8a1a\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 228f1d5c at:\n",
            "https://smith.langchain.com/o/9e230557-3355-5562-8dbc-5c44b3821cac/datasets/ddde42ac-bb21-4605-8c21-d8758fecbf2d\n",
            "[------------------------------------------------->] 8/8"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>47d14dd7-0cc9-4bd0-aab4-cd8b92763dde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.136286</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.665415</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.800291</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.200322</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.667953</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.482421</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.807849</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      8     0        8.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       4   NaN             NaN   \n",
              "mean                     NaN   NaN        7.136286   \n",
              "std                      NaN   NaN        3.665415   \n",
              "min                      NaN   NaN        2.800291   \n",
              "25%                      NaN   NaN        5.200322   \n",
              "50%                      NaN   NaN        6.667953   \n",
              "75%                      NaN   NaN        7.482421   \n",
              "max                      NaN   NaN       14.807849   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      8  \n",
              "unique                                     8  \n",
              "top     47d14dd7-0cc9-4bd0-aab4-cd8b92763dde  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - dfb6ce7b',\n",
              " 'results': {'d9d46438-c73d-4013-8c8c-fcd4e74fe90a': {'input': {'question': 'What is a Large Language Model (LLM)?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1249728a-4f10-4255-82f1-f56734a13637'), target_run_id=None)],\n",
              "   'execution_time': 6.81802,\n",
              "   'run_id': '47d14dd7-0cc9-4bd0-aab4-cd8b92763dde',\n",
              "   'output': \"A Large Language Model (LLM) is a type of artificial intelligence model designed to understand and generate human language. These models are typically based on deep learning techniques, particularly neural networks, and are trained on vast amounts of text data. Here are some key characteristics and features of LLMs:\\n\\n1. **Scale**: LLMs are characterized by their large number of parameters, often in the billions or even trillions. This large scale allows them to capture a wide range of linguistic patterns and nuances.\\n\\n2. **Training Data**: They are trained on diverse and extensive datasets that include books, articles, websites, and other text sources. This helps them learn the structure, grammar, and context of human language.\\n\\n3. **Capabilities**: LLMs can perform a variety of language-related tasks, such as text generation, translation, summarization, question answering, and sentiment analysis. They can also engage in conversations and provide coherent and contextually relevant responses.\\n\\n4. **Pre-training and Fine-tuning**: LLMs are typically pre-trained on a large corpus of text in an unsupervised manner, learning general language patterns. They can then be fine-tuned on specific tasks or domains with additional, more focused training data.\\n\\n5. **Applications**: LLMs are used in numerous applications, including chatbots, virtual assistants, content creation, code generation, and more. They are also employed in research and development for advancing natural language processing (NLP) technologies.\\n\\n6. **Examples**: Some well-known LLMs include OpenAI's GPT-3, Google's BERT, and Facebook's RoBERTa.\\n\\nOverall, LLMs represent a significant advancement in the field of NLP, enabling machines to better understand and generate human language with a high degree of fluency and accuracy.\",\n",
              "   'reference': {'must_mention': ['neural', 'natural language']}},\n",
              "  'ad47a5c5-5900-4fe5-a241-726b246a59c8': {'input': {'question': 'What are some common use cases of LLMs?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ffbb3b6c-1746-48b8-bdf5-a36c4b9cbccd'), target_run_id=None)],\n",
              "   'execution_time': 6.745028,\n",
              "   'run_id': 'ab36fa60-0989-487f-a860-0b1b55c0b37d',\n",
              "   'output': 'Large Language Models (LLMs) have a wide range of applications across various domains. Some common use cases include:\\n\\n1. **Natural Language Processing (NLP) Tasks:**\\n   - **Text Generation:** Creating human-like text for chatbots, content creation, and storytelling.\\n   - **Translation:** Translating text from one language to another.\\n   - **Summarization:** Condensing long documents into shorter summaries.\\n   - **Sentiment Analysis:** Determining the sentiment or emotion behind a piece of text.\\n   - **Named Entity Recognition (NER):** Identifying and classifying entities (like names, dates, and locations) in text.\\n\\n2. **Customer Support:**\\n   - **Chatbots:** Providing automated responses to customer queries.\\n   - **Virtual Assistants:** Assisting users with tasks and answering questions.\\n\\n3. **Content Creation:**\\n   - **Blog Posts and Articles:** Generating content for websites and blogs.\\n   - **Social Media Posts:** Crafting posts for platforms like Twitter, Facebook, and LinkedIn.\\n   - **Marketing Copy:** Writing promotional material and advertisements.\\n\\n4. **Education and Training:**\\n   - **Tutoring Systems:** Offering personalized tutoring and answering student questions.\\n   - **Content Summarization:** Summarizing educational materials and research papers.\\n\\n5. **Healthcare:**\\n   - **Medical Records Analysis:** Extracting and summarizing information from medical records.\\n   - **Patient Interaction:** Assisting with patient queries and providing information.\\n\\n6. **Research and Development:**\\n   - **Literature Review:** Summarizing and analyzing academic papers.\\n   - **Data Analysis:** Assisting with the interpretation of complex datasets.\\n\\n7. **Entertainment:**\\n   - **Script Writing:** Assisting in writing scripts for movies, TV shows, and video games.\\n   - **Interactive Storytelling:** Creating dynamic and interactive stories.\\n\\n8. **Legal and Compliance:**\\n   - **Document Review:** Analyzing legal documents and contracts.\\n   - **Compliance Monitoring:** Ensuring adherence to regulations and standards.\\n\\n9. **Business Intelligence:**\\n   - **Report Generation:** Creating business reports and executive summaries.\\n   - **Data Insights:** Extracting insights from business data.\\n\\n10. **Personal Use:**\\n    - **Email Drafting:** Assisting in writing and responding to emails.\\n    - **Personal Assistants:** Managing schedules, reminders, and personal tasks.\\n\\nThese are just a few examples, and the potential applications of LLMs continue to grow as the technology advances.',\n",
              "   'reference': {'must_mention': ['text generation', 'summarization']}},\n",
              "  'c8a2450e-21a4-4151-994a-d211de43dd9b': {'input': {'question': 'Which transformer architecture is most commonly used in LLMs?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d513480e-7632-42be-9b2e-b77c722499d3'), target_run_id=None)],\n",
              "   'execution_time': 2.800291,\n",
              "   'run_id': 'b947fc71-fba2-4b17-805e-2f5b4e168da2',\n",
              "   'output': 'The transformer architecture most commonly used in large language models (LLMs) is the \"Transformer\" model introduced by Vaswani et al. in the paper \"Attention is All You Need.\" This architecture is the foundation for many state-of-the-art LLMs, including:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers)**: Uses the transformer encoder architecture.\\n2. **GPT (Generative Pre-trained Transformer)**: Uses the transformer decoder architecture.\\n3. **T5 (Text-To-Text Transfer Transformer)**: Uses both the encoder and decoder parts of the transformer architecture.\\n\\nThe key components of the transformer architecture include self-attention mechanisms, multi-head attention, and feed-forward neural networks, which allow these models to handle long-range dependencies and parallelize training effectively.',\n",
              "   'reference': {'must_mention': ['GPT', 'BERT']}},\n",
              "  '4ee89991-2911-4901-8974-9aa429afd5f6': {'input': {'question': 'What is fine-tuning in the context of LLMs?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8695d46e-8d93-4bbc-b705-d5a2e05bd6b5'), target_run_id=None)],\n",
              "   'execution_time': 4.378252,\n",
              "   'run_id': 'd70fc219-52db-4ded-b7c1-5730a17d1a07',\n",
              "   'output': 'Fine-tuning in the context of Large Language Models (LLMs) refers to the process of taking a pre-trained language model and further training it on a specific dataset that is typically smaller and more specialized than the dataset used for the initial training. This process allows the model to adapt to specific tasks, domains, or styles, improving its performance on those particular tasks.\\n\\nHere are the key steps involved in fine-tuning an LLM:\\n\\n1. **Pre-training**: The language model is initially trained on a large and diverse corpus of text data. This helps the model learn general language patterns, grammar, facts about the world, and some reasoning abilities.\\n\\n2. **Dataset Preparation**: A specialized dataset relevant to the specific task or domain is prepared. This dataset is usually labeled and much smaller than the dataset used for pre-training.\\n\\n3. **Fine-tuning**: The pre-trained model is further trained on the specialized dataset. During this phase, the model adjusts its parameters to better fit the specific characteristics and requirements of the new dataset.\\n\\n4. **Evaluation and Iteration**: The fine-tuned model is evaluated on a validation set to ensure it performs well on the specific task. Based on the performance, further adjustments and iterations may be made.\\n\\nFine-tuning is particularly useful because it leverages the general language understanding acquired during pre-training and adapts it to specific needs, often resulting in better performance on specialized tasks compared to training a model from scratch.',\n",
              "   'reference': {'must_mention': ['pre-trained', 'specific tasks']}},\n",
              "  'd972ee28-3fc1-4f16-b1ae-e43f6c2eeaa4': {'input': {'question': 'Who are some key contributors to the development of LLMs?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7191ae6c-ff7c-4ae1-af8b-312a093140d3'), target_run_id=None)],\n",
              "   'execution_time': 9.475624,\n",
              "   'run_id': '8a817439-a4c3-44e9-9d06-9e5b89bc4791',\n",
              "   'output': 'The development of Large Language Models (LLMs) has been influenced by numerous key contributors from both academic and industry backgrounds. Here are some notable contributors and their contributions:\\n\\n1. **Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Min Yang, Wenbin Zhang**:\\n   - These authors have contributed to the understanding of the history, development, and principles of LLMs through their survey paper titled \"History, Development, and Principles of Large Language Models-An Introductory Survey.\"\\n\\n2. **Santhanakrishnan Anand, Ofer Arazy, Narayan B. Mandayam, Oded Nov**:\\n   - Their work focuses on the dynamics of collaborative authoring and the interplay between individual contributors and community mechanisms, which can be relevant to the collaborative efforts in developing LLMs.\\n\\n3. **Brihat Sharma, Yanjun Gao, Timothy Miller, Matthew M. Churpek, Majid Afshar, Dmitriy Dligach**:\\n   - These researchers have worked on multi-task training with in-domain language models for diagnostic reasoning, highlighting the importance of domain-specific training in enhancing the performance of LLMs.\\n\\n4. **Jiawen Liu, Haoxiang Zhang, Ying Zou**:\\n   - Their research aims to understand the profiles and engagement of contributors in popular machine learning libraries, which is crucial for the development and maintenance of LLMs.\\n\\nAdditionally, some key figures and organizations in the broader AI and machine learning community have made significant contributions to the development of LLMs:\\n\\n- **OpenAI**: Known for developing models like GPT-3 and GPT-4.\\n- **Google Research**: Contributions include the development of the Transformer architecture and BERT.\\n- **DeepMind**: Known for advancements in AI research, including language models.\\n- **Facebook AI Research (FAIR)**: Contributions to various aspects of machine learning and AI, including language models.\\n\\nThese contributors and organizations have played pivotal roles in advancing the field of LLMs through research, development, and practical applications.',\n",
              "   'reference': {'must_mention': ['OpenAI', 'Google', 'Hugging Face']}},\n",
              "  'fbfe34ba-6695-4682-a015-bfec649ee9d5': {'input': {'question': 'What is the significance of tokenization in LLMs?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9ffa3afc-48ef-46db-be8c-0e80ecb87212'), target_run_id=None)],\n",
              "   'execution_time': 6.590877,\n",
              "   'run_id': 'efb8164f-d4f5-4439-8466-edaffbb91c1d',\n",
              "   'output': 'Tokenization is a crucial preprocessing step in the functioning of Large Language Models (LLMs) like GPT-3, BERT, and others. Here are some key points highlighting its significance:\\n\\n1. **Text Representation**: Tokenization converts raw text into a format that can be processed by the model. It breaks down the text into smaller units called tokens, which can be words, subwords, or characters. This allows the model to handle and understand the text more effectively.\\n\\n2. **Handling Vocabulary**: By breaking text into tokens, LLMs can manage a vast vocabulary. Instead of learning representations for every possible word, subword tokenization (like Byte Pair Encoding or WordPiece) allows the model to handle rare or unseen words by breaking them into known subword units.\\n\\n3. **Efficiency**: Tokenization helps in reducing the complexity of the input data. By converting text into tokens, the model can process the data more efficiently, leading to faster training and inference times.\\n\\n4. **Context Understanding**: Tokenization helps in preserving the context of the text. Proper tokenization ensures that meaningful units of text are kept together, which is essential for the model to understand and generate coherent and contextually relevant responses.\\n\\n5. **Handling Different Languages**: Tokenization is particularly important for multilingual models. It allows the model to handle different languages and scripts by breaking down text into a common set of tokens, facilitating cross-lingual understanding and generation.\\n\\n6. **Reducing Ambiguity**: Proper tokenization helps in reducing ambiguity in text. For example, tokenizing \"can\\'t\" into \"can\" and \"\\'t\" helps the model understand the negation, which is crucial for accurate language understanding and generation.\\n\\n7. **Alignment with Model Architecture**: The architecture of LLMs, such as transformers, is designed to work with tokenized input. Tokenization aligns the input text with the model\\'s requirements, ensuring that the data is in the right format for processing.\\n\\nIn summary, tokenization is a fundamental step that enables LLMs to process, understand, and generate human language effectively. It transforms raw text into a structured format that the model can work with, ensuring efficient and accurate language modeling.',\n",
              "   'reference': {'must_mention': ['splitting', 'subwords']}},\n",
              "  '96238e6d-8776-44f2-a9ad-d3ff6981cf5a': {'input': {'question': 'What is the primary challenge of scaling LLMs?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('df4008ec-ec4f-40f2-b0a8-9d97fca89f13'), target_run_id=None)],\n",
              "   'execution_time': 5.474345,\n",
              "   'run_id': '3de4f489-94a2-45a4-ad14-fbc2d0c795c3',\n",
              "   'output': 'The primary challenge of scaling Large Language Models (LLMs) involves several key factors:\\n\\n1. **Computational Resources**: Scaling LLMs requires significant computational power, including high-performance GPUs or TPUs, large amounts of memory, and extensive storage capabilities. The cost and availability of these resources can be a major bottleneck.\\n\\n2. **Training Time**: Training large models can take weeks or even months, depending on the size of the model and the dataset. This extended training time can delay deployment and increase costs.\\n\\n3. **Data Requirements**: LLMs require vast amounts of high-quality data to train effectively. Gathering, cleaning, and preprocessing this data can be a substantial challenge.\\n\\n4. **Energy Consumption**: The energy required to train and run large models is considerable, raising concerns about environmental impact and sustainability.\\n\\n5. **Model Optimization**: As models grow in size, optimizing them for performance and efficiency becomes increasingly complex. This includes challenges in parallelizing training across multiple devices and managing communication overhead.\\n\\n6. **Inference Latency**: Larger models can lead to increased inference times, which can be problematic for real-time applications. Optimizing inference to maintain acceptable latency is a significant challenge.\\n\\n7. **Overfitting and Generalization**: Ensuring that larger models generalize well to new, unseen data without overfitting to the training data is a critical concern.\\n\\n8. **Interpretability and Debugging**: As models become more complex, understanding their behavior and debugging issues becomes more difficult. This can hinder the ability to trust and verify model outputs.\\n\\n9. **Ethical and Bias Concerns**: Larger models can amplify biases present in the training data, leading to ethical concerns and the need for robust bias detection and mitigation strategies.\\n\\n10. **Deployment and Maintenance**: Deploying and maintaining large models in production environments requires robust infrastructure and ongoing monitoring to ensure performance and reliability.\\n\\nAddressing these challenges requires advancements in hardware, algorithms, data management, and ethical practices.',\n",
              "   'reference': {'must_mention': ['computational', 'memory']}},\n",
              "  'c0d5d15b-0064-4cbd-ad0f-e187fc30be81': {'input': {'question': 'How do LLMs handle context in conversations?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('29b4ea40-63e3-4d2f-a858-4c3320facd73'), target_run_id=None)],\n",
              "   'execution_time': 14.807849,\n",
              "   'run_id': '5b2372b8-c998-43c9-ae54-93a16635b3d7',\n",
              "   'output': 'Large Language Models (LLMs) handle context in conversations through several mechanisms, primarily involving the use of attention mechanisms and memory. Here are some key points on how they manage context:\\n\\n1. **Attention Mechanisms**: \\n   - **Self-Attention**: In models like Transformers, self-attention mechanisms allow the model to weigh the importance of different words in a sentence relative to each other. This helps the model understand the context by focusing on relevant parts of the input text.\\n   - **Contextual Embeddings**: Words are represented as vectors that capture their meanings in context. These embeddings are updated at each layer of the model to reflect the context in which the words appear.\\n\\n2. **Memory**:\\n   - **Recurrent Neural Networks (RNNs)**: Earlier models like RNNs and LSTMs (Long Short-Term Memory networks) maintain a hidden state that is updated as each word is processed, allowing the model to remember previous words in the sequence.\\n   - **Transformers**: Transformers, which are the basis for many modern LLMs, do not rely on sequential processing. Instead, they use positional encodings to keep track of the order of words and self-attention to maintain context over long sequences.\\n\\n3. **Tokenization**:\\n   - **Subword Tokenization**: LLMs often use subword tokenization methods like Byte Pair Encoding (BPE) or WordPiece to break down words into smaller units. This helps the model handle rare words and understand the context better by focusing on meaningful subword units.\\n\\n4. **Context Windows**:\\n   - **Fixed-Length Context Windows**: LLMs process text in chunks of a fixed length, known as context windows. The size of this window determines how much context the model can consider at once. For example, GPT-3 has a context window of 2048 tokens.\\n   - **Sliding Windows**: For longer texts, a sliding window approach can be used where the model processes overlapping chunks of text to maintain context over longer passages.\\n\\n5. **Fine-Tuning and Pre-Training**:\\n   - **Pre-Training**: LLMs are pre-trained on large corpora of text to learn general language patterns and context handling.\\n   - **Fine-Tuning**: They can be fine-tuned on specific datasets to improve their ability to handle context in particular domains or types of conversations.\\n\\n6. **Prompt Engineering**:\\n   - **Prompt Design**: The way prompts are designed can help LLMs maintain context. For example, including relevant previous conversation turns in the prompt can help the model generate more contextually appropriate responses.\\n\\n7. **Memory-Augmented Models**:\\n   - **External Memory**: Some advanced models incorporate external memory mechanisms that allow them to store and retrieve information over longer conversations, improving their ability to maintain context over extended interactions.\\n\\nThese mechanisms collectively enable LLMs to handle context in conversations, allowing them to generate coherent and contextually appropriate responses.',\n",
              "   'reference': {'must_mention': ['context window', 'previous inputs']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### This code snippet creates a state graph for managing the flow of an AI agent within the AgentState. The graph consists of two nodes: \"agent\", which is linked to the call_model function responsible for invoking the AI model, and \"action\", connected to the tool_node function, which likely handles a specific action or tool execution. This structure allows the agent to first make decisions based on the model's output and then perform a subsequent action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### This code sets the entry point of the graph_with_helpfulness_check state graph to the \"agent\" node. This means that when the graph is executed, it will start by invoking the call_model function associated with the \"agent\" node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### This code defines a function tool_call_or_helpful that decides the next action in a process based on the current state of an AI interaction. It first checks if a tool has been called; if so, it returns \"action\". If there are more than 10 messages, it returns \"END\". Otherwise, it evaluates the helpfulness of the AI's final response compared to the initial query using a prompt template and a GPT-4 model. Depending on the evaluation, it either ends the process (\"end\") or continues (\"continue\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### \n",
        "STEPS: \n",
        "\n",
        "1. The function looks at the last message in the conversation to see if the AI used a tool, like a search or an API call.\n",
        "\n",
        "\n",
        "2. If the AI did use a tool, the function instructs the process to take an \"action\" based on that tool's result.\n",
        "\n",
        "2. If no tool was used, the function then checks how many messages have been exchanged so far.\n",
        "\n",
        "4. If the conversation has more than 10 messages, the function decides to \"end\" the conversation to prevent it from getting too long.\n",
        "\n",
        "5. If the message count is 10 or fewer, the function prepares to evaluate how helpful the AI's last response was by creating a prompt.\n",
        "\n",
        "6. The function sends this prompt to GPT-4, asking whether the AI's final response was helpful or not.\n",
        "\n",
        "7. Based on GPT-4's answer, the function either decides to \"end\" the conversation if the response was helpful, or to \"continue\" if it wasn't, allowing for further refinement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### This code adds a connection, or \"edge,\" between two nodes in the graph_with_helpfulness_check state graph. Specifically, it links the \"action\" node to the \"agent\" node, allowing the process to move from the action phase back to the agent phase. This creates a loop where, after performing an action, the graph can return to the agent for further processing or decision-making."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### This code compiles the graph_with_helpfulness_check into a runnable form and stores it in the variable agent_with_helpfulness_check. This compiled version of the graph can now be executed, enabling the agent to follow the defined sequence of nodes and edges when processing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### This code sends a set of inputs to the agent_with_helpfulness_check for processing. The input is a message asking three questions related to machine learning. The code runs asynchronously, processing the input through the agent in chunks. For each chunk, it prints updates on which node in the graph is currently active ('agent', 'action', etc.) and displays the messages generated by that node. This allows you to see the agent's step-by-step progress in handling the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "22470df4-9aa7-4751-95b6-d30ce71b17db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_a9jNKGzuZ7lIQR77m0eSkqki', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_VHJBlnCTZVrmtmxN645PE1aS', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_Ki4k2cSXtFd95uENfkcLRTs3', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 171, 'total_tokens': 247}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c6c417bf-985a-4c4c-86de-d588ca3d6afd-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_a9jNKGzuZ7lIQR77m0eSkqki', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_VHJBlnCTZVrmtmxN645PE1aS', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_Ki4k2cSXtFd95uENfkcLRTs3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 76, 'total_tokens': 247})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content=\"Let's jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning ($\\\\\\\\approx$100K prompt-response ... For example, on the subset of the GLUE dataset with T5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as Llama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05% on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to ... LoRA's approach to decomposing ( Œî W ) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model's learning capability is preserved with significantly ... A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016; Cai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many deep learning tasks, especially those with a heavily over-parametrized neural network, the learned neural network will enjoy low-rank properties after ...\", name='duckduckgo_search', tool_call_id='call_a9jNKGzuZ7lIQR77m0eSkqki'), ToolMessage(content=\"Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public. If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.‚Ä¶ Tim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan Alistarh Published: 16 Jan 2024, Last Modified: 14 Mar 2024 ICLR 2024 poster Everyone Revisions BibTeX QLORA: efficient finetuning of quantized LLMs AUTHORs: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer Authors Info & Claims NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems Article No.: 441, Pages 10088 - 10115 Published: 30 May 2024 Publication History 0 0 Publisher Site\", name='duckduckgo_search', tool_call_id='call_VHJBlnCTZVrmtmxN645PE1aS'), ToolMessage(content='Learn how attention mechanisms in deep learning enable models to focus on relevant information and improve performance in tasks such as machine translation, image captioning, and speech recognition. Understand the steps and components of attention mechanism architecture and see examples of its applications. Attention mechanism is a fundamental invention in artificial intelligence and machine learning, redefining the capabilities of deep learning models. This mechanism, inspired by the human mental process of selective focus, has emerged as a pillar in a variety of applications, accelerating developments in natural language processing, computer vision, and beyond. There are several types of attention mechanisms, each designed to cater to specific use cases. Here are a few notable ones: 1. Self-Attention Mechanism. Self-attention, also known as intra ... They all use transformer architecture with attention mechanisms at their core to solve problems across domains. In the Transformer series, we go over the ingredients that have made Transformers a universal recipe for machine learning. First up, we take a visual dive to understand the attention mechanism: Why transformers and attention took over. The concept of \"attention\" in deep learning has its roots in the effort to improve Recurrent Neural Networks (RNNs) for handling longer sequences or sentences. For instance, consider translating a sentence from one language to another. Translating a sentence word-by-word is usually not an option because it ignores the complex grammatical ...', name='duckduckgo_search', tool_call_id='call_Ki4k2cSXtFd95uENfkcLRTs3')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"### LoRA in Machine Learning\\n**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning method for large language models. Instead of fine-tuning all the parameters of a pre-trained model, LoRA focuses on training only low-rank perturbations to selected weight matrices. This approach significantly reduces memory usage and computational requirements while maintaining the model's performance. LoRA is particularly useful for adapting large pre-trained models to new tasks efficiently.\\n\\n### Tim Dettmers\\n**Tim Dettmers** is a Ph.D. student at the Allen School and a leading academic expert on quantization and the use of deep learning hardware accelerators. He is known for his work on QLoRA, a novel approach to fine-tuning pre-trained models that drastically reduces the GPU memory required. His research aims to make cutting-edge machine learning techniques more accessible and efficient.\\n\\n### Attention in Machine Learning\\n**Attention mechanisms** are a fundamental invention in artificial intelligence and machine learning, enabling models to focus on relevant information and improve performance in various tasks such as machine translation, image captioning, and speech recognition. Inspired by the human mental process of selective focus, attention mechanisms have become a cornerstone in natural language processing, computer vision, and other domains. There are several types of attention mechanisms, including:\\n\\n1. **Self-Attention**: Also known as intra-attention, it allows a model to attend to different positions of a single sequence to compute a representation of the sequence.\\n2. **Multi-Head Attention**: Extends self-attention by using multiple attention heads to capture different aspects of the information.\\n\\nAttention mechanisms are integral to the transformer architecture, which has revolutionized the field of deep learning by enabling models to handle longer sequences and more complex tasks effectively.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 357, 'prompt_tokens': 1249, 'total_tokens': 1606}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-784fcfb0-bd53-43b6-af27-3ab11f0b035b-0', usage_metadata={'input_tokens': 1249, 'output_tokens': 357, 'total_tokens': 1606})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "0593c6f5-0a1d-41f6-960b-f32d101b3ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt engineering is a concept primarily associated with the field of artificial intelligence, particularly in the context of natural language processing (NLP) and large language models like GPT-3. It involves the design and crafting of prompts (input text) to elicit desired responses from AI models. The goal is to optimize the input to get the most accurate, relevant, or useful output from the model.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Crafting Effective Prompts**: Designing questions or statements that guide the AI to produce the best possible response.\n",
            "2. **Iterative Testing**: Continuously refining prompts based on the responses received to improve the quality of the output.\n",
            "3. **Understanding Model Behavior**: Knowing how different models respond to various types of input to tailor prompts accordingly.\n",
            "4. **Applications**: Used in chatbots, virtual assistants, automated content generation, and more.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts highlighted the importance of crafting effective prompts to harness their full potential. The concept, however, has roots in earlier AI and NLP research but gained significant attention and formalization with the rise of these advanced models.\n",
            "\n",
            "To get more detailed and up-to-date information, I can perform a search. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "RAG, or Retrieval-Augmented Generation, is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The core idea is to retrieve relevant documents or pieces of information from a large corpus and use this retrieved information to guide the generation process, typically using models like GPT-3 or BERT.\n",
            "\n",
            "RAG became prominent in the NLP community around 2020, particularly with the publication of the paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" by researchers from Facebook AI Research (FAIR). This paper demonstrated how RAG could be used to improve performance on various knowledge-intensive tasks, such as open-domain question answering and fact-checking, by leveraging both retrieval and generation capabilities.\n",
            "\n",
            "Would you like more detailed information or specific papers on RAG?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new, often smaller, dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks.\n",
            "\n",
            "### Key Points of Fine-Tuning:\n",
            "1. **Pre-trained Model**: Start with a model that has been trained on a large dataset.\n",
            "2. **New Dataset**: Use a smaller, task-specific dataset to further train the model.\n",
            "3. **Adaptation**: The model adjusts its parameters to better perform the new task.\n",
            "4. **Efficiency**: Reduces the need for large amounts of data and computational resources compared to training a model from scratch.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and the development of large-scale neural networks. It gained significant attention with the success of models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which demonstrated the effectiveness of fine-tuning in natural language processing tasks.\n",
            "\n",
            "To pinpoint the exact timeline and significant milestones, I can look up relevant articles and papers. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents, or Large Language Model-based agents, are systems that utilize large language models to perform tasks, make decisions, and interact with users. These agents leverage the capabilities of advanced language models to understand and generate human-like text, enabling them to handle a variety of tasks such as answering questions, providing recommendations, and even performing complex decision-making processes.\n",
            "\n",
            "### Emergence and Development:\n",
            "1. **Early Development**: The concept of using language models for tasks has been around for a while, but the significant breakthrough came with the development of large-scale models like GPT-3 by OpenAI, which was released in June 2020. These models demonstrated unprecedented capabilities in understanding and generating human-like text.\n",
            "\n",
            "2. **Advancements**: Following GPT-3, there have been continuous advancements in the field, with newer models like GPT-4 and other large language models being developed by various organizations. These models have been integrated into various applications, leading to the creation of LLM-based agents.\n",
            "\n",
            "3. **Applications**: LLM-based agents have found applications in numerous fields, including customer service, content creation, education, and more. They are used to automate tasks, provide personalized recommendations, and assist in decision-making processes.\n",
            "\n",
            "4. **Research and Development**: Ongoing research focuses on improving the autonomy, scalability, and effectiveness of these agents. This includes developing frameworks for better decision-making, enhancing interaction capabilities, and ensuring the agents can handle long-horizon tasks that require extensive planning and execution.\n",
            "\n",
            "In summary, LLM-based agents broke onto the scene around 2020 with the release of GPT-3 and have since evolved rapidly, finding applications across various domains and continuing to improve in capability and performance.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
